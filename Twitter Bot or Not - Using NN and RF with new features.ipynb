{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Twitter Bots Using Machine Learning\n",
    "\n",
    "\n",
    "### In this notebook we build a RF classifier with a different subset of features, and also implement a Multilayer Perceptron NN.\n",
    "\n",
    "The predictions from this model will be fed into the final pipeline, presented in the Expert's Advice ipython notebook.\n",
    "\n",
    "\n",
    "Anantha Natarajn Selvaganapathy<br/>\n",
    "N16989511<br/>\n",
    "ans599<br/>\n",
    "http://ananth.co.in\n",
    "\n",
    "The objective of this project is to use machine learning techniques to detect weather a given Twitter account is a bot or not. \n",
    "\n",
    "We will be using various machine learning algorithms and compare and analyze their predictions. We will also explore the use of deep learning techniques and compare their results with regression and classification algorithms. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin, we import all the required libraries. We will be using sklearn for all the machine learning models, and pandas and numpy for data manipulation and cleaning.\n",
    "\n",
    "We also use matplotlib and seaborn for plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the datasets into a pandas dataframe. The `describe` method gives us a quick glimpse of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot data shape: (1056, 20)\n",
      "Non bot data shape: (1176, 20)\n",
      "Index(['id', 'id_str', 'screen_name', 'location', 'description', 'url',\n",
      "       'followers_count', 'friends_count', 'listedcount', 'created_at',\n",
      "       'favourites_count', 'verified', 'statuses_count', 'lang', 'status',\n",
      "       'default_profile', 'default_profile_image', 'has_extended_profile',\n",
      "       'name', 'bot'],\n",
      "      dtype='object')\n",
      "BOT DATA:\n",
      "                 id  followers_count  friends_count   listedcount  \\\n",
      "count  1.056000e+03     1.056000e+03    1056.000000   1056.000000   \n",
      "mean   2.527841e+17     1.557358e+04    1353.546402    129.678977   \n",
      "std    3.709023e+17     2.402553e+05   12972.825548    890.896498   \n",
      "min    2.425231e+06     0.000000e+00       0.000000      0.000000   \n",
      "25%    2.434705e+09     1.075000e+01       1.000000      0.000000   \n",
      "50%    3.029672e+09     9.950000e+01      14.000000     10.000000   \n",
      "75%    7.562500e+17     6.012500e+02     259.250000     31.000000   \n",
      "max    8.410000e+17     7.154703e+06  355287.000000  15866.000000   \n",
      "\n",
      "       favourites_count  statuses_count     bot  \n",
      "count       1056.000000    1.056000e+03  1056.0  \n",
      "mean         522.598485    2.995948e+04     1.0  \n",
      "std         4872.668894    2.399779e+05     0.0  \n",
      "min            0.000000    0.000000e+00     1.0  \n",
      "25%            0.000000    8.275000e+01     1.0  \n",
      "50%            1.000000    1.595500e+03     1.0  \n",
      "75%           27.000000    9.721000e+03     1.0  \n",
      "max       112382.000000    6.863363e+06     1.0  \n",
      "\n",
      "NON BOT DATA:\n",
      "                 id  followers_count  friends_count    listedcount  \\\n",
      "count  1.176000e+03     1.176000e+03    1176.000000    1176.000000   \n",
      "mean   5.893622e+16     1.825315e+06    5676.700680    5594.161565   \n",
      "std    2.089459e+17     7.026759e+06   50666.751345   16950.004532   \n",
      "min    3.526000e+03     0.000000e+00       0.000000       0.000000   \n",
      "25%    2.605315e+07     1.517500e+02     124.750000       2.000000   \n",
      "50%    9.403230e+07     4.352500e+03     363.500000     110.500000   \n",
      "75%    5.505657e+08     4.164525e+05     953.750000    3206.750000   \n",
      "max    8.410000e+17     9.632156e+07  978964.000000  222411.000000   \n",
      "\n",
      "       favourites_count  statuses_count     bot  \n",
      "count       1176.000000     1176.000000  1176.0  \n",
      "mean        3326.715136    10790.845238     0.0  \n",
      "std        23471.219509    31545.476492     0.0  \n",
      "min            0.000000        0.000000     0.0  \n",
      "25%           17.000000      306.500000     0.0  \n",
      "50%          229.000000     2845.500000     0.0  \n",
      "75%         1676.500000     9927.000000     0.0  \n",
      "max       714021.000000   741849.000000     0.0  \n",
      "\n",
      "NEWBOT DATA:\n",
      "(2797, 20)\n"
     ]
    }
   ],
   "source": [
    "newbot_data = pd.read_csv('./training_data_2_csv_UTF.csv', encoding = \"utf-8\")\n",
    "\n",
    "bot_data = pd.read_csv('bots_data.csv', encoding = \"ISO-8859-1\")\n",
    "nonbot_data = pd.read_csv('nonbots_data.csv', encoding = \"ISO-8859-1\")\n",
    "\n",
    "print(\"Bot data shape:\", bot_data.shape)\n",
    "print(\"Non bot data shape:\", nonbot_data.shape)\n",
    "\n",
    "print(nonbot_data.columns)\n",
    "\n",
    "print(\"BOT DATA:\")\n",
    "print(bot_data.describe())\n",
    "\n",
    "print(\"\\nNON BOT DATA:\")\n",
    "print(nonbot_data.describe())\n",
    "\n",
    "print(\"\\nNEWBOT DATA:\")\n",
    "print(newbot_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frames = [ newbot_data]\n",
    "dataset = pd.concat(frames)\n",
    "\n",
    "df1 = pd.concat(frames)\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    " \n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000) \n",
    "\n",
    "dataset['location'] = dataset['location'].fillna('none')\n",
    "train_data_features = vectorizer.fit_transform(dataset['location'])\n",
    "\n",
    "train_data_features = train_data_features.toarray()\n",
    "train_data_features = pd.DataFrame(data=train_data_features[:,:],    \n",
    "              index=train_data_features[:,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names = dataset['screen_name']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset['status'] = dataset['status'].apply(lambda x: int(len(str(x))))\n",
    "\n",
    "dataset['screen_name'] = dataset['screen_name'].apply(lambda x: 'bot' in x.lower())\n",
    "dataset['description'] = dataset['description'].apply(lambda x: 'bot' in str(x).lower())\n",
    "dataset['name'] = dataset['name'].apply(lambda x: int('bot' in str(x).lower()))\n",
    "\n",
    "dataset = shuffle(dataset)\n",
    "\n",
    "randomForrest = RandomForestClassifier(n_estimators=100, \n",
    "                                       min_samples_split=5, \n",
    "                                       random_state=0)\n",
    "\n",
    "X = dataset[['screen_name', 'description','followers_count', 'friends_count', \n",
    "             'listedcount', 'favourites_count', 'verified', \n",
    "             'statuses_count', 'status', 'default_profile', \n",
    "             'default_profile_image']]\n",
    "\n",
    "y = dataset[['bot']]\n",
    "\n",
    "x_testing = X[-200:]\n",
    "y_testing = y[-200:]\n",
    "X = X[:-200]\n",
    "y = y[:-200]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictX = pd.read_csv('./test_data_4_students.csv', encoding = \"ISO-8859-1\")\n",
    "\n",
    "\n",
    "x_test = predictX[['screen_name', 'description','followers_count', 'friends_count', \n",
    "             'listed_count', 'favorites_count', 'verified', \n",
    "             'statuses_count', 'status', 'default_profile',\n",
    "             'default_profile_image', 'name']]\n",
    "\n",
    "x_test = x_test[:575]\n",
    "\n",
    "x_test['screen_name'] = x_test['screen_name'].apply(lambda x: int('bot' in x.lower()))\n",
    "x_test['description'] = x_test['description'].apply(lambda x: int('bot' in str(x).lower()))\n",
    "x_test['listed_count'] = x_test['listed_count'].apply(lambda x: 0 if str(x) == \"None\" else int(x))\n",
    "x_test['favorites_count'] = x_test['favorites_count'].apply(lambda x: 0 if str(x) == \"None\" else int(x))\n",
    "x_test['followers_count'] = x_test['followers_count'].apply(lambda x: 0 if str(x) == \"None\" else int(x))\n",
    "x_test['friends_count'] = x_test['friends_count'].apply(lambda x: 0 if str(x) == \"None\" else int(x))\n",
    "\n",
    "x_test['verified'] = x_test['verified'].apply(lambda x: 1 if str(x) == \"TRUE\" else 0)\n",
    "x_test['statuses_count'] = x_test['statuses_count'].apply(lambda x: int(x))\n",
    "x_test['status'] = x_test['status'].apply(lambda x: int(len(str(x))))\n",
    "x_test['default_profile'] = x_test['default_profile'].apply(lambda x: 1 if x == \"TRUE\" else 0)\n",
    "x_test['default_profile_image'] = x_test['default_profile_image'].apply(lambda x: 1 if x == \"TRUE\" else 0)\n",
    "x_test['name'] = x_test['name'].apply(lambda x: int('bot' in x.lower()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.855\n"
     ]
    }
   ],
   "source": [
    "#Gradient Boosted Classifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf = GradientBoostingClassifier(n_estimators=200, learning_rate=0.99900, \n",
    "                                 max_depth=15, random_state=1, min_samples_split=12).fit(X.values, y.values.ravel())\n",
    "\n",
    "print(clf.score(x_testing, y_testing))\n",
    "# pred2 = clf.predict(x_test.values)\n",
    "# ids = predictX['id'][:575]\n",
    "# with open('ans599_3_usingnewdata.csv', 'w') as the_file:\n",
    "#     the_file.write(\"id,bot\\n\")    \n",
    "#     for i in range(575):\n",
    "#         the_file.write(str(int(ids[i])) +\",\"+ str(pred2[i])+\"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.912200954279\n",
      "0.88\n"
     ]
    }
   ],
   "source": [
    "randomForrest.fit(X.values, y.values.ravel())\n",
    "\n",
    "# pred = randomForrest.predict(x_test.values)\n",
    "# ids = predictX['id'][:575]\n",
    "# with open('ans599_2_usingBothdata_useName.csv', 'w') as the_file:\n",
    "#     the_file.write(\"id,bot\\n\")    \n",
    "#     for i in range(575):\n",
    "#         the_file.write(str(int(ids[i])) +\",\"+ str(pred[i])+\"\\n\") \n",
    "\n",
    "scores = cross_val_score(randomForrest, X.values, y.values.ravel())\n",
    "print (scores.mean())\n",
    "\n",
    "pred = randomForrest.predict(x_testing)\n",
    "y_test = y_testing.values\n",
    "# y_test = y_testing.ravel()\n",
    "print(accuracy_score(y_testing, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Result\n",
    "\n",
    "The EF classifier with ['screen_name', 'description','followers_count', 'friends_count', \n",
    "             'listed_count', 'favorites_count', 'verified', \n",
    "             'statuses_count', 'status', 'default_profile',\n",
    "             'default_profile_image', 'name'] features gives us 0.91 - 0.92 cross validation accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-layer Perceptron regressor\n",
    "\n",
    "This model optimizes the squared-loss using LBFGS or stochastic gradient descent.\n",
    "\n",
    "I experimented with 15-25 layers with 33 neurons per layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[282   0]\n",
      " [218   0]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      1.00      0.72       282\n",
      "          1       0.00      0.00      0.00       218\n",
      "\n",
      "avg / total       0.32      0.56      0.41       500\n",
      "\n",
      "Index(['id', 'id_str', 'screen_name', 'location', 'description', 'url',\n",
      "       'followers_count', 'friends_count', 'listed_count', 'created_at',\n",
      "       'favorites_count', 'verified', 'statuses_count', 'lang', 'status',\n",
      "       'default_profile', 'default_profile_image', 'has_extended_profile',\n",
      "       'name', 'bot'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sananth12/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.909442140651\n",
      "0.922\n",
      "Wrote to file!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "frames = [ newbot_data]\n",
    "dataset = pd.concat(frames)\n",
    "\n",
    "dataset['status'] = dataset['status'].apply(lambda x: int(len(str(x))))\n",
    "dataset['status_hasBot'] = dataset['status'].apply(lambda x: 'bot' in str(x).lower())\n",
    "\n",
    "dataset['screen_name'] = dataset['screen_name'].apply(lambda x: 'bot' in x.lower())\n",
    "dataset['description'] = dataset['description'].apply(lambda x: 'bot' in str(x).lower())\n",
    "dataset['name'] = dataset['name'].apply(lambda x: int('bot' in str(x).lower()))\n",
    "dataset[\"has_extended_profile\"] = dataset[\"has_extended_profile\"].apply(lambda x:  0 if str(x) == \"False\" else 1)\n",
    "dataset[\"has_extended_profile\"].fillna(0)\n",
    "dataset['location'] = dataset['location'].apply(lambda x: 1 if len(str(x)) > 0 else 0)\n",
    "\n",
    "dataset = shuffle(dataset)\n",
    "\n",
    "\n",
    "X = dataset[['screen_name', 'description','followers_count', 'friends_count', \n",
    "             'listedcount', 'favourites_count', 'verified', 'name',\n",
    "             'statuses_count', 'status', 'default_profile', 'has_extended_profile', 'location', 'status_hasBot',\n",
    "             'default_profile_image']]\n",
    "\n",
    "y = dataset[['bot']]\n",
    "\n",
    "x_testing = X[-500:]\n",
    "y_testing = y[-500:]\n",
    "X = X[:-500]\n",
    "y_train = y[:-500]\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# Fit only to the training data\n",
    "scaler.fit(X)\n",
    "\n",
    "X_train = scaler.transform(X)\n",
    "X_test = scaler.transform(x_testing)\n",
    "\n",
    "# X_train\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(33,33,33,33,33,33,33,33,33,33,33,33,33,33,33, 2))\n",
    "mlp.fit(X_train,y_train.values.ravel())\n",
    "\n",
    "predictions = mlp.predict(X_test)\n",
    "print(confusion_matrix(y_testing,predictions))\n",
    "\n",
    "print(classification_report(y_testing,predictions))\n",
    "\n",
    "predictX = pd.read_csv('./test_data_4_students.csv', encoding = \"ISO-8859-1\")\n",
    "\n",
    "x_test = predictX[['screen_name', 'description','followers_count', 'friends_count', \n",
    "             'listed_count', 'favorites_count', 'verified', \n",
    "             'statuses_count', 'status', 'default_profile', 'has_extended_profile', 'location', 'name',\n",
    "             'default_profile_image']]\n",
    "\n",
    "\n",
    "x_test = x_test[:575]\n",
    "print(predictX.columns)\n",
    "x_test['screen_name'] = x_test['screen_name'].apply(lambda x: int('bot' in str(x).lower()))\n",
    "x_test['description'] = x_test['description'].apply(lambda x: int('bot' in str(x).lower()))\n",
    "x_test['listed_count'] = x_test['listed_count'].apply(lambda x: 0 if str(x) == \"None\" else int(x))\n",
    "x_test['favorites_count'] = x_test['favorites_count'].apply(lambda x: 0 if str(x) == \"None\" else int(x))\n",
    "x_test['followers_count'] = x_test['followers_count'].apply(lambda x: 0 if str(x) == \"None\" else int(x))\n",
    "x_test['friends_count'] = x_test['friends_count'].apply(lambda x: 0 if str(x) == \"None\" else int(x))\n",
    "\n",
    "x_test['verified'] = x_test['verified'].apply(lambda x: 1 if str(x) == \"TRUE\" else 0)\n",
    "x_test['statuses_count'] = x_test['statuses_count'].apply(lambda x: int(x))\n",
    "x_test['status'] = x_test['status'].apply(lambda x: int(len(str(x))))\n",
    "x_test['default_profile'] = x_test['default_profile'].apply(lambda x: 1 if x == \"TRUE\" else 0)\n",
    "x_test['default_profile_image'] = x_test['default_profile_image'].apply(lambda x: 1 if x == \"TRUE\" else 0)\n",
    "x_test['name'] = x_test['name'].apply(lambda x: int('bot' in str(x).lower()))\n",
    "x_test['status_hasBot'] = x_test['status'].apply(lambda x: int('bot' in str(x).lower()))\n",
    "x_test['location'] = x_test['location'].apply(lambda x: 1 if len(str(x)) > 0 else 0)\n",
    "\n",
    "x_test[\"has_extended_profile\"] = x_test[\"has_extended_profile\"].apply(lambda x:  0 if str(x) == \"False\" else 1)\n",
    "x_test[\"has_extended_profile\"].fillna(0)\n",
    "\n",
    "randomForrest = RandomForestClassifier(n_estimators=200, \n",
    "                                       min_samples_split=2, \n",
    "                                       random_state=1, max_depth=13)\n",
    "\n",
    "randomForrest.fit(X, y_train.values.ravel())\n",
    "\n",
    "scores = cross_val_score(randomForrest, X, y_train.values.ravel())\n",
    "print (scores.mean())\n",
    "predictions = randomForrest.predict(x_testing)\n",
    "print(accuracy_score(y_testing, predictions))\n",
    "\n",
    "x_test.dtypes\n",
    "pred = randomForrest.predict(x_test)\n",
    "ids = predictX['id'][:575]\n",
    "with open('ans599__1_useAllFea.csv', 'w') as the_file:\n",
    "    the_file.write(\"id,bot\\n\")    \n",
    "    for i in range(575):\n",
    "        the_file.write(str(int(ids[i])) +\",\"+ str(pred[i])+\"\\n\")\n",
    "print(\"Wrote to file!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'id_str', 'screen_name', 'location', 'description', 'url',\n",
       "       'followers_count', 'friends_count', 'listedcount', 'created_at',\n",
       "       'favourites_count', 'verified', 'statuses_count', 'lang', 'status',\n",
       "       'default_profile', 'default_profile_image', 'has_extended_profile',\n",
       "       'name', 'bot', 'status_hasBot'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102  old: 1 new: 0 tune: 1 allf: 0  vote:3\n",
      "103  old: 0 new: 1 tune: 0 allf: 1  vote:2\n",
      "144  old: 0 new: 1 tune: 0 allf: 0  vote:1\n",
      "159  old: 1 new: 0 tune: 1 allf: 1  vote:3\n",
      "281  old: 0 new: 1 tune: 0 allf: 0  vote:1\n",
      "288  old: 1 new: 0 tune: 1 allf: 1  vote:4\n",
      "299  old: 1 new: 0 tune: 1 allf: 1  vote:4\n",
      "401  old: 0 new: 1 tune: 0 allf: 0  vote:1\n",
      "441  old: 1 new: 0 tune: 1 allf: 1  vote:4\n",
      "483  old: 1 new: 1 tune: 0 allf: 0  vote:2\n",
      "493  old: 0 new: 1 tune: 0 allf: 0  vote:1\n",
      "494  old: 0 new: 1 tune: 0 allf: 0  vote:1\n",
      "495  old: 0 new: 1 tune: 0 allf: 1  vote:2\n",
      "496  old: 0 new: 1 tune: 0 allf: 1  vote:2\n",
      "537  old: 0 new: 1 tune: 0 allf: 1  vote:2\n",
      "539  old: 0 new: 1 tune: 0 allf: 0  vote:1\n",
      "557  old: 0 new: 1 tune: 0 allf: 0  vote:1\n",
      "261\n",
      "270\n",
      "252\n",
      "283\n"
     ]
    }
   ],
   "source": [
    "old_result = pd.read_csv('ans599_1.csv')\n",
    "new_result = pd.read_csv('ans599_2_tunePrams_addLocNameUrl_idLen.csv')\n",
    "new2_result = pd.read_csv('ans599_1_tunePrams.csv')\n",
    "allf_result = pd.read_csv('ans599__1_useAllFea.csv')\n",
    "name_result = pd.read_csv('ans599_2_usingBothdata_useName.csv')\n",
    "\n",
    "# merged = old_result.merge(new_result, indicator=True, how='outer')\n",
    "# merged[merged['_merge'] == 'right_only']\n",
    "a = old_result[\"bot\"]\n",
    "b = new_result[\"bot\"]\n",
    "c = new2_result[\"bot\"]\n",
    "d = allf_result[\"bot\"]\n",
    "e = name_result[\"bot\"]\n",
    "\n",
    "for i in range(575):\n",
    "    vote = a[i]+b[i]+c[i]+d[i]+e[i]\n",
    "    vb = 0 if vote <= 2 else 1\n",
    "    if vb != b[i]:\n",
    "        print( str(i)+ \"  old: \"+str(a[i])+\" new: \"+str(b[i])+ \" tune: \"+ str(c[i])+ \" allf: \"+ str(d[i]) +\"  vote:\"+str(vote))\n",
    "print(sum(a))\n",
    "print(sum(b))\n",
    "print(sum(c))\n",
    "print(sum(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Understanding the differences in models\n",
    "\n",
    "We can see from above that the predictions match for more than 95% data points.\n",
    "\n",
    "We will be using the output of this model, and other generated models along with agressive tuning to come to a vote. This approach is called Expert's Advice, other also known as Randomized weighted majority algorithm.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Randomized_weighted_majority_algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
